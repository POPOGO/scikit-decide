{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from typing import Callable\n",
        "\n",
        "from skdecide.hub.domain.gym import GymPlanningDomain, GymWidthDomain, GymDiscreteActionDomain\n",
        "from skdecide.hub.solver.iw import IW\n",
        "from skdecide.utils import rollout"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select a [Gym environment](https://gym.openai.com/envs) and horizon parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ENV_NAME = 'MountainCar-v0'\n",
        "HORIZON = 500"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a specific IW domain by combining Gym domain templates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class D(GymPlanningDomain, GymWidthDomain, GymDiscreteActionDomain):\n",
        "    pass\n",
        "\n",
        "\n",
        "class GymIWDomain(D):\n",
        "    \"\"\"This class wraps a cost-based deterministic OpenAI Gym environment as a domain\n",
        "        usable by a width-based planner\n",
        "\n",
        "    !!! warning\n",
        "        Using this class requires OpenAI Gym to be installed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gym_env: gym.Env,\n",
        "                       set_state: Callable[[gym.Env, D.T_memory[D.T_state]], None] = None,\n",
        "                       get_state: Callable[[gym.Env], D.T_memory[D.T_state]] = None,\n",
        "                       termination_is_goal: bool = True,\n",
        "                       continuous_feature_fidelity: int = 1,\n",
        "                       discretization_factor: int = 3,\n",
        "                       branching_factor: int = None,\n",
        "                       max_depth: int = 50) -> None:\n",
        "        \"\"\"Initialize GymIWDomain.\n",
        "\n",
        "        # Parameters\n",
        "        gym_env: The deterministic Gym environment (gym.env) to wrap.\n",
        "        set_state: Function to call to set the state of the gym environment.\n",
        "                   If None, default behavior is to deepcopy the environment when changing state\n",
        "        get_state: Function to call to get the state of the gym environment.\n",
        "                   If None, default behavior is to deepcopy the environment when changing state\n",
        "        termination_is_goal: True if the termination condition is a goal (and not a dead-end)\n",
        "        continuous_feature_fidelity: Number of integers to represent a continuous feature\n",
        "                                     in the interval-based feature abstraction (higher is more precise)\n",
        "        discretization_factor: Number of discretized action variable values per continuous action variable\n",
        "        branching_factor: if not None, sample branching_factor actions from the resulting list of discretized actions\n",
        "        max_depth: maximum depth of states to explore from the initial state\n",
        "        \"\"\"\n",
        "        GymPlanningDomain.__init__(self,\n",
        "                                   gym_env=gym_env,\n",
        "                                   set_state=set_state,\n",
        "                                   get_state=get_state,\n",
        "                                   termination_is_goal=termination_is_goal,\n",
        "                                   max_depth=max_depth)\n",
        "        GymDiscreteActionDomain.__init__(self,\n",
        "                                         discretization_factor=discretization_factor,\n",
        "                                         branching_factor=branching_factor)\n",
        "        GymWidthDomain.__init__(self, continuous_feature_fidelity=continuous_feature_fidelity)\n",
        "        gym_env._max_episode_steps = max_depth"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Solve the domain with IW solver in \"realtime\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "domain_factory = lambda: GymIWDomain(gym_env=gym.make(ENV_NAME),\n",
        "                                     termination_is_goal=True,\n",
        "                                     continuous_feature_fidelity=1,\n",
        "                                     discretization_factor=3,\n",
        "                                     max_depth=HORIZON)\n",
        "domain = domain_factory()\n",
        "\n",
        "if IW.check_domain(domain):\n",
        "    solver_factory = lambda: IW(domain_factory=domain_factory,\n",
        "                                state_features=lambda d, s: d.bee1_features(\n",
        "                                                                np.append(\n",
        "                                                                    s._state,\n",
        "                                                                    s._context[3].value.reward if s._context[3] is not None else 0)),\n",
        "                                use_state_feature_hash=False,\n",
        "                                node_ordering=lambda a_gscore, a_novelty, a_depth, b_gscore, b_novelty, b_depth: a_novelty > b_novelty,\n",
        "                                parallel=False, debug_logs=False)\n",
        "\n",
        "    with solver_factory() as solver:\n",
        "        GymIWDomain.solve_with(solver, domain_factory)\n",
        "        rollout(domain, solver, num_episodes=1, max_steps=HORIZON, max_framerate=30,\n",
        "                outcome_formatter=lambda o: f'{o.observation} - cost: {o.value.cost:.2f}')\n",
        "        # value, steps = simple_rollout(domain_factory(), solver, HORIZON)\n",
        "        # print('value:', value)\n",
        "        # print('steps:', steps)\n",
        "        print('explored:', solver.get_nb_of_explored_states())\n",
        "        print('pruned:', solver.get_nb_of_pruned_states())\n",
        "        filter_intermediate_scores = []\n",
        "        current_score = None\n",
        "        for score in solver.get_intermediate_scores():\n",
        "            if current_score is None or current_score != score[2]:\n",
        "                current_score = score[2]\n",
        "                filter_intermediate_scores.append(score)\n",
        "        print('Intermediate scores:' + str(filter_intermediate_scores))"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}